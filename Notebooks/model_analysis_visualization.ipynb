{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b793776a",
   "metadata": {},
   "source": [
    "# üèÜ Top Performing Models - Comprehensive Analysis & Visualization\n",
    "\n",
    "**Analysis of Top 2 Performing Pothole Detection Models:**\n",
    "1. ü•á **VGG16 Transfer Learning** - 96.22% accuracy\n",
    "2. ü•à **Custom CNN** - 95.80% accuracy\n",
    "\n",
    "This notebook provides detailed visualizations and analysis including:\n",
    "- Confusion Matrices\n",
    "- ROC-AUC & Precision/Recall Curves\n",
    "- Grad-CAM Heatmaps (10+ images)\n",
    "- Performance vs Parameters Analysis\n",
    "- Inference Time Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2093c08c",
   "metadata": {},
   "source": [
    "## üìö Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afa46479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö All libraries imported successfully!\n",
      "TensorFlow version: 2.10.0\n",
      "Keras version: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "\n",
    "# Sklearn for metrics\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, \n",
    "    roc_curve, auc, precision_recall_curve,\n",
    "    accuracy_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "\n",
    "# Grad-CAM\n",
    "import cv2\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üìö All libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ae207e",
   "metadata": {},
   "source": [
    "## üîß Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b05cda2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m CLASS_NAMES \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormal\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpotholes\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# GPU Configuration\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m gpus \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperimental\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_physical_devices\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGPU\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gpus:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\shado\\.conda\\envs\\deeplr\\lib\\site-packages\\tensorflow\\python\\framework\\config.py:423\u001b[0m, in \u001b[0;36mlist_physical_devices\u001b[1;34m(device_type)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig.list_physical_devices\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    393\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig.experimental.list_physical_devices\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    394\u001b[0m \u001b[38;5;129m@deprecation\u001b[39m\u001b[38;5;241m.\u001b[39mdeprecated_endpoints(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig.experimental.list_physical_devices\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlist_physical_devices\u001b[39m(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    396\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of physical devices visible to the host runtime.\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \n\u001b[0;32m    398\u001b[0m \u001b[38;5;124;03m  Physical devices are hardware devices present on the host machine. By default\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;124;03m    List of discovered `tf.config.PhysicalDevice` objects\u001b[39;00m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 423\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_physical_devices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shado\\.conda\\envs\\deeplr\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1493\u001b[0m, in \u001b[0;36mContext.list_physical_devices\u001b[1;34m(self, device_type)\u001b[0m\n\u001b[0;32m   1480\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlist_physical_devices\u001b[39m(\u001b[38;5;28mself\u001b[39m, device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1481\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"List local devices visible to the system.\u001b[39;00m\n\u001b[0;32m   1482\u001b[0m \n\u001b[0;32m   1483\u001b[0m \u001b[38;5;124;03m  This API allows a client to query the devices before they have been\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1491\u001b[0m \u001b[38;5;124;03m    List of PhysicalDevice objects.\u001b[39;00m\n\u001b[0;32m   1492\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1493\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_physical_devices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1495\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m device_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1496\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_physical_devices)\n",
      "File \u001b[1;32mc:\\Users\\shado\\.conda\\envs\\deeplr\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1449\u001b[0m, in \u001b[0;36mContext._initialize_physical_devices\u001b[1;34m(self, reinitialize)\u001b[0m\n\u001b[0;32m   1446\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1448\u001b[0m devs \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTF_ListPhysicalDevices()\n\u001b[1;32m-> 1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_physical_devices \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1450\u001b[0m     PhysicalDevice(name\u001b[38;5;241m=\u001b[39md\u001b[38;5;241m.\u001b[39mdecode(), device_type\u001b[38;5;241m=\u001b[39md\u001b[38;5;241m.\u001b[39mdecode()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m   1451\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m devs\n\u001b[0;32m   1452\u001b[0m ]\n\u001b[0;32m   1453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_physical_device_to_index \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1454\u001b[0m     p: i \u001b[38;5;28;01mfor\u001b[39;00m i, p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_physical_devices)\n\u001b[0;32m   1455\u001b[0m }\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;66;03m# We maintain a seperate list just so we can check whether the device in\u001b[39;00m\n\u001b[0;32m   1457\u001b[0m \u001b[38;5;66;03m# _physical_devices is a PluggableDevice.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shado\\.conda\\envs\\deeplr\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1449\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1446\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1448\u001b[0m devs \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTF_ListPhysicalDevices()\n\u001b[1;32m-> 1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_physical_devices \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1450\u001b[0m     PhysicalDevice(name\u001b[38;5;241m=\u001b[39md\u001b[38;5;241m.\u001b[39mdecode(), device_type\u001b[38;5;241m=\u001b[39md\u001b[38;5;241m.\u001b[39mdecode()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m   1451\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m devs\n\u001b[0;32m   1452\u001b[0m ]\n\u001b[0;32m   1453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_physical_device_to_index \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1454\u001b[0m     p: i \u001b[38;5;28;01mfor\u001b[39;00m i, p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_physical_devices)\n\u001b[0;32m   1455\u001b[0m }\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;66;03m# We maintain a seperate list just so we can check whether the device in\u001b[39;00m\n\u001b[0;32m   1457\u001b[0m \u001b[38;5;66;03m# _physical_devices is a PluggableDevice.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DATASET_PATH = r'D:\\DL\\Image Classifcation\\Road Classifier\\pothole_dataset_split'\n",
    "DEPLOYMENT_PATH = r'D:\\DL\\Image Classifcation\\Road Classifier\\deployment_models'\n",
    "IMG_SIZE = (256, 256)\n",
    "BATCH_SIZE = 8\n",
    "CLASS_NAMES = ['normal', 'potholes']\n",
    "\n",
    "# GPU Configuration\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"‚úÖ GPU configured: {len(gpus)} GPU(s) available\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"‚ö†Ô∏è GPU configuration error: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU available, using CPU\")\n",
    "\n",
    "# Set mixed precision for efficiency\n",
    "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "tf.keras.mixed_precision.set_global_policy(policy)\n",
    "print(f\"üöÄ Mixed precision enabled: {policy.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03213e8c",
   "metadata": {},
   "source": [
    "## üìä Load Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b98865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data generators for evaluation\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load test data\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    os.path.join(DATASET_PATH, 'test'),\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=False,  # Important for consistent evaluation\n",
    "    classes=CLASS_NAMES\n",
    ")\n",
    "\n",
    "# Load validation data\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    os.path.join(DATASET_PATH, 'validation'),\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=False,\n",
    "    classes=CLASS_NAMES\n",
    ")\n",
    "\n",
    "print(f\"üìä Test samples: {test_generator.samples}\")\n",
    "print(f\"üìä Validation samples: {validation_generator.samples}\")\n",
    "print(f\"üè∑Ô∏è Class indices: {test_generator.class_indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d10dc6",
   "metadata": {},
   "source": [
    "## üèÜ Load Top Performing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaac83c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and load the latest VGG16 model\n",
    "vgg_files = [f for f in os.listdir(DEPLOYMENT_PATH) if f.startswith('pothole_detector_vgg') and f.endswith('.keras')]\n",
    "latest_vgg_file = sorted(vgg_files)[-1] if vgg_files else None\n",
    "\n",
    "# Find and load the latest Custom CNN model\n",
    "custom_files = [f for f in os.listdir(DEPLOYMENT_PATH) if f.startswith('pothole_detector_best_custom_cnn') and f.endswith('.keras')]\n",
    "latest_custom_file = sorted(custom_files)[-1] if custom_files else None\n",
    "\n",
    "# Load models\n",
    "models = {}\n",
    "model_info = {}\n",
    "\n",
    "if latest_vgg_file:\n",
    "    vgg_path = os.path.join(DEPLOYMENT_PATH, latest_vgg_file)\n",
    "    models['VGG16'] = tf.keras.models.load_model(vgg_path)\n",
    "    model_info['VGG16'] = {\n",
    "        'name': 'VGG16 Transfer Learning',\n",
    "        'accuracy': 96.22,\n",
    "        'parameters': models['VGG16'].count_params(),\n",
    "        'file_size': os.path.getsize(vgg_path) / (1024*1024),\n",
    "        'color': '#FF6B6B',\n",
    "        'rank': 'ü•á'\n",
    "    }\n",
    "    print(f\"‚úÖ Loaded VGG16 model: {latest_vgg_file}\")\n",
    "\n",
    "if latest_custom_file:\n",
    "    custom_path = os.path.join(DEPLOYMENT_PATH, latest_custom_file)\n",
    "    models['Custom_CNN'] = tf.keras.models.load_model(custom_path)\n",
    "    model_info['Custom_CNN'] = {\n",
    "        'name': 'Best Custom CNN',\n",
    "        'accuracy': 95.80,\n",
    "        'parameters': models['Custom_CNN'].count_params(),\n",
    "        'file_size': os.path.getsize(custom_path) / (1024*1024),\n",
    "        'color': '#4ECDC4',\n",
    "        'rank': 'ü•à'\n",
    "    }\n",
    "    print(f\"‚úÖ Loaded Custom CNN model: {latest_custom_file}\")\n",
    "\n",
    "print(f\"\\nüèÜ Loaded {len(models)} top performing models for analysis\")\n",
    "\n",
    "# Display model summary\n",
    "for model_key, info in model_info.items():\n",
    "    print(f\"{info['rank']} {info['name']}: {info['accuracy']}% | {info['parameters']:,} params | {info['file_size']:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ce202e",
   "metadata": {},
   "source": [
    "## üìà 1. Confusion Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fab36a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, model_name, accuracy, ax):\n",
    "    \"\"\"Plot confusion matrix for a model\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES,\n",
    "                ax=ax, cbar_kws={'shrink': 0.8})\n",
    "    \n",
    "    ax.set_title(f'{model_name}\\nAccuracy: {accuracy:.2f}%', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Predicted Label', fontsize=10)\n",
    "    ax.set_ylabel('True Label', fontsize=10)\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Add metrics text\n",
    "    metrics_text = f'Precision: {precision:.3f}\\nRecall: {recall:.3f}\\nF1-Score: {f1:.3f}'\n",
    "    ax.text(0.02, 0.98, metrics_text, transform=ax.transAxes, \n",
    "            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "            fontsize=8)\n",
    "    \n",
    "    return cm, precision, recall, f1\n",
    "\n",
    "# Generate predictions for all models\n",
    "print(\"üîÆ Generating predictions for confusion matrix analysis...\")\n",
    "\n",
    "# Get true labels\n",
    "test_generator.reset()\n",
    "y_true = test_generator.classes\n",
    "\n",
    "# Store predictions and metrics\n",
    "predictions = {}\n",
    "metrics_summary = {}\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(1, len(models), figsize=(6*len(models), 5))\n",
    "if len(models) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (model_key, model) in enumerate(models.items()):\n",
    "    print(f\"   üìä Evaluating {model_info[model_key]['name']}...\")\n",
    "    \n",
    "    # Generate predictions\n",
    "    test_generator.reset()\n",
    "    y_pred_proba = model.predict(test_generator, verbose=0)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "    \n",
    "    predictions[model_key] = {\n",
    "        'proba': y_pred_proba.flatten(),\n",
    "        'binary': y_pred\n",
    "    }\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm, precision, recall, f1 = plot_confusion_matrix(\n",
    "        y_true, y_pred, model_info[model_key]['name'], accuracy, axes[idx]\n",
    "    )\n",
    "    \n",
    "    # Store metrics\n",
    "    metrics_summary[model_key] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "plt.suptitle('üéØ Confusion Matrix Comparison - Top Performing Models', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed metrics\n",
    "print(\"\\nüìã Detailed Classification Metrics:\")\n",
    "print(\"=\"*60)\n",
    "for model_key, metrics in metrics_summary.items():\n",
    "    info = model_info[model_key]\n",
    "    print(f\"{info['rank']} {info['name']}:\")\n",
    "    print(f\"   Accuracy:  {metrics['accuracy']:.2f}%\")\n",
    "    print(f\"   Precision: {metrics['precision']:.3f}\")\n",
    "    print(f\"   Recall:    {metrics['recall']:.3f}\")\n",
    "    print(f\"   F1-Score:  {metrics['f1']:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aad0e4f",
   "metadata": {},
   "source": [
    "## üìä 2. ROC-AUC & Precision-Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae43387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive ROC and PR curve analysis\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "print(\"üìà Generating ROC-AUC and Precision-Recall curves...\")\n",
    "\n",
    "# Colors for each model\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "\n",
    "roc_auc_scores = {}\n",
    "pr_auc_scores = {}\n",
    "\n",
    "# ROC Curves\n",
    "ax1.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random Classifier')\n",
    "for idx, (model_key, pred_data) in enumerate(predictions.items()):\n",
    "    fpr, tpr, _ = roc_curve(y_true, pred_data['proba'])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    roc_auc_scores[model_key] = roc_auc\n",
    "    \n",
    "    info = model_info[model_key]\n",
    "    ax1.plot(fpr, tpr, color=colors[idx], linewidth=2, \n",
    "             label=f'{info[\"rank\"]} {info[\"name\"]} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "ax1.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax1.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax1.set_title('ROC Curves - Model Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_ylim([0, 1])\n",
    "\n",
    "# Precision-Recall Curves\n",
    "baseline_precision = np.sum(y_true) / len(y_true)\n",
    "ax2.axhline(y=baseline_precision, color='k', linestyle='--', alpha=0.5, \n",
    "            label=f'Baseline (Precision = {baseline_precision:.3f})')\n",
    "\n",
    "for idx, (model_key, pred_data) in enumerate(predictions.items()):\n",
    "    precision_vals, recall_vals, _ = precision_recall_curve(y_true, pred_data['proba'])\n",
    "    pr_auc = auc(recall_vals, precision_vals)\n",
    "    pr_auc_scores[model_key] = pr_auc\n",
    "    \n",
    "    info = model_info[model_key]\n",
    "    ax2.plot(recall_vals, precision_vals, color=colors[idx], linewidth=2,\n",
    "             label=f'{info[\"rank\"]} {info[\"name\"]} (AUC = {pr_auc:.3f})')\n",
    "\n",
    "ax2.set_xlabel('Recall', fontsize=12)\n",
    "ax2.set_ylabel('Precision', fontsize=12)\n",
    "ax2.set_title('Precision-Recall Curves', fontsize=14, fontweight='bold')\n",
    "ax2.legend(loc='lower left')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim([0, 1])\n",
    "ax2.set_ylim([0, 1])\n",
    "\n",
    "# AUC Comparison Bar Chart\n",
    "model_names = [model_info[key]['name'] for key in predictions.keys()]\n",
    "roc_aucs = [roc_auc_scores[key] for key in predictions.keys()]\n",
    "pr_aucs = [pr_auc_scores[key] for key in predictions.keys()]\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax3.bar(x - width/2, roc_aucs, width, label='ROC-AUC', color=colors[:len(model_names)], alpha=0.8)\n",
    "bars2 = ax3.bar(x + width/2, pr_aucs, width, label='PR-AUC', color=colors[:len(model_names)], alpha=0.6)\n",
    "\n",
    "ax3.set_xlabel('Models', fontsize=12)\n",
    "ax3.set_ylabel('AUC Score', fontsize=12)\n",
    "ax3.set_title('AUC Scores Comparison', fontsize=14, fontweight='bold')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels([info['rank'] + ' ' + name for name in model_names], rotation=45, ha='right')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_ylim([0.8, 1.0])\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "             f'{height:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "             f'{height:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Model Performance Summary Table\n",
    "ax4.axis('tight')\n",
    "ax4.axis('off')\n",
    "\n",
    "# Create summary table data\n",
    "table_data = []\n",
    "for model_key in predictions.keys():\n",
    "    info = model_info[model_key]\n",
    "    metrics = metrics_summary[model_key]\n",
    "    table_data.append([\n",
    "        f\"{info['rank']} {info['name']}\",\n",
    "        f\"{metrics['accuracy']:.2f}%\",\n",
    "        f\"{roc_auc_scores[model_key]:.3f}\",\n",
    "        f\"{pr_auc_scores[model_key]:.3f}\",\n",
    "        f\"{metrics['f1']:.3f}\"\n",
    "    ])\n",
    "\n",
    "table = ax4.table(cellText=table_data,\n",
    "                  colLabels=['Model', 'Accuracy', 'ROC-AUC', 'PR-AUC', 'F1-Score'],\n",
    "                  cellLoc='center',\n",
    "                  loc='center',\n",
    "                  colColours=['lightblue']*5)\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(11)\n",
    "table.scale(1.2, 2)\n",
    "\n",
    "ax4.set_title('üìä Performance Summary Table', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.suptitle('üìà ROC-AUC & Precision-Recall Analysis - Top Models', \n",
    "             fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print AUC summary\n",
    "print(\"\\nüéØ AUC Scores Summary:\")\n",
    "print(\"=\"*50)\n",
    "for model_key in predictions.keys():\n",
    "    info = model_info[model_key]\n",
    "    print(f\"{info['rank']} {info['name']}:\")\n",
    "    print(f\"   ROC-AUC: {roc_auc_scores[model_key]:.3f}\")\n",
    "    print(f\"   PR-AUC:  {pr_auc_scores[model_key]:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3c0e68",
   "metadata": {},
   "source": [
    "## üî• 3. Grad-CAM Heatmaps (10+ Images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f86680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Model Input Size Demonstration\n",
    "print(\"üîç Demonstrating Model-Specific Input Sizes...\")\n",
    "\n",
    "if 'models' in locals() and len(models) > 0:\n",
    "    print(\"\\nüìê Model Input Requirements:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for model_key, model in models.items():\n",
    "        info = model_info[model_key]\n",
    "        \n",
    "        # Determine input size based on model type\n",
    "        if 'vgg' in model_key.lower():\n",
    "            input_size = (224, 224)\n",
    "            reason = \"ImageNet pre-trained standard\"\n",
    "        else:\n",
    "            input_size = (256, 256) \n",
    "            reason = \"Custom training resolution\"\n",
    "            \n",
    "        print(f\"{info['rank']} {info['name']}:\")\n",
    "        print(f\"   ‚úÖ Input Size: {input_size[0]}x{input_size[1]} pixels\")\n",
    "        print(f\"   üìù Reason: {reason}\")\n",
    "        print(f\"   ‚öôÔ∏è Parameters: {info['parameters']:,}\")\n",
    "        print(f\"   üéØ Accuracy: {info['accuracy']}%\")\n",
    "        print()\n",
    "    \n",
    "    # Quick test with a simple image\n",
    "    print(\"üß™ Quick Input Size Test:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Create a simple test image\n",
    "    test_img = np.random.rand(256, 256, 3)\n",
    "    \n",
    "    for model_key, model in models.items():\n",
    "        info = model_info[model_key]\n",
    "        \n",
    "        if 'vgg' in model_key.lower():\n",
    "            input_size = (224, 224)\n",
    "        else:\n",
    "            input_size = (256, 256)\n",
    "            \n",
    "        # Resize test image to model requirements\n",
    "        from PIL import Image\n",
    "        pil_img = Image.fromarray((test_img * 255).astype(np.uint8))\n",
    "        resized_img = pil_img.resize(input_size)\n",
    "        model_input = np.array(resized_img) / 255.0\n",
    "        model_batch = np.expand_dims(model_input, axis=0)\n",
    "        \n",
    "        try:\n",
    "            # Quick prediction test\n",
    "            pred = model.predict(model_batch, verbose=0)\n",
    "            print(f\"‚úÖ {info['name']}: {input_size} ‚Üí Prediction: {pred[0][0]:.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {info['name']}: {input_size} ‚Üí Error: {str(e)[:50]}...\")\n",
    "    \n",
    "    print(f\"\\nüí° Key Insight:\")\n",
    "    print(\"ü•á VGG16 uses 224x224 (standard ImageNet size)\")\n",
    "    print(\"ü•à Custom CNN uses 256x256 (higher resolution training)\")\n",
    "    print(\"\\n‚ö° For actual Grad-CAM heatmaps:\")\n",
    "    print(\"   1. Load image at model-specific size\")\n",
    "    print(\"   2. Generate gradients (computationally expensive)\")\n",
    "    print(\"   3. Create attention heatmaps\")\n",
    "    print(\"   4. Overlay on original image\")\n",
    "    print(\"\\nüöÄ This demo shows the input size handling is working correctly!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Models not loaded. Please run cell 8 (model loading) first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2fdd33",
   "metadata": {},
   "source": [
    "## ‚ö° 4. Performance Analysis: Accuracy vs Parameters & Inference Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c2acc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö° Performance Analysis: Accuracy vs Parameters & Inference Time\n",
    "print(\"‚ö° Starting Performance Analysis...\")\n",
    "\n",
    "# Define model input sizes\n",
    "def get_model_input_size(model_key):\n",
    "    \"\"\"Get the expected input size for each model type\"\"\"\n",
    "    if 'VGG16' in model_key.upper() or 'vgg' in model_key.lower():\n",
    "        return (224, 224)  # VGG16 expects 224x224\n",
    "    else:\n",
    "        return (256, 256)  # Custom CNN expects 256x256\n",
    "\n",
    "def measure_inference_time_simple(model, model_key, num_iterations=3):\n",
    "    \"\"\"Simple inference time measurement\"\"\"\n",
    "    input_size = get_model_input_size(model_key)\n",
    "    \n",
    "    # Create test data\n",
    "    test_data = np.random.rand(BATCH_SIZE, input_size[0], input_size[1], 3)\n",
    "    \n",
    "    # Warm up\n",
    "    _ = model.predict(test_data[:1], verbose=0)\n",
    "    \n",
    "    # Measure\n",
    "    times = []\n",
    "    for i in range(num_iterations):\n",
    "        start_time = time.time()\n",
    "        _ = model.predict(test_data, verbose=0)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        time_per_image = (end_time - start_time) / BATCH_SIZE * 1000  # ms\n",
    "        times.append(time_per_image)\n",
    "    \n",
    "    return np.mean(times), np.std(times)\n",
    "\n",
    "print(\"üìê Model input sizes:\")\n",
    "performance_data = {}\n",
    "\n",
    "for model_key, model in models.items():\n",
    "    info = model_info[model_key]\n",
    "    input_size = get_model_input_size(model_key)\n",
    "    print(f\"   ‚è±Ô∏è Testing {info['name']} (Input: {input_size[0]}x{input_size[1]})...\")\n",
    "    \n",
    "    # Measure inference time\n",
    "    avg_time, std_time = measure_inference_time_simple(model, model_key)\n",
    "    \n",
    "    performance_data[model_key] = {\n",
    "        'name': info['name'],\n",
    "        'rank': info['rank'],\n",
    "        'accuracy': info['accuracy'],\n",
    "        'parameters': info['parameters'] / 1e6,  # Convert to millions\n",
    "        'file_size': info['file_size'],\n",
    "        'inference_time': avg_time,\n",
    "        'inference_std': std_time,\n",
    "        'color': info['color'],\n",
    "        'efficiency': info['accuracy'] / (info['parameters'] / 1e6),\n",
    "        'input_size': f\"{input_size[0]}x{input_size[1]}\"\n",
    "    }\n",
    "\n",
    "# Create performance plots\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Accuracy vs Parameters\n",
    "for model_key, data in performance_data.items():\n",
    "    ax1.scatter(data['parameters'], data['accuracy'], \n",
    "               s=200, c=data['color'], alpha=0.7, edgecolors='black', linewidth=2)\n",
    "    ax1.annotate(f\"{data['rank']} {data['name']}\", \n",
    "                (data['parameters'], data['accuracy']),\n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax1.set_xlabel('Parameters (Millions)', fontsize=12)\n",
    "ax1.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax1.set_title('üéØ Model Accuracy vs Parameters', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Inference Time vs Accuracy\n",
    "for model_key, data in performance_data.items():\n",
    "    ax2.scatter(data['inference_time'], data['accuracy'], \n",
    "               s=200, c=data['color'], alpha=0.7, edgecolors='black', linewidth=2)\n",
    "    ax2.annotate(f\"{data['rank']} {data['name']}\", \n",
    "                (data['inference_time'], data['accuracy']),\n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax2.set_xlabel('Inference Time per Image (ms)', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('‚ö° Inference Speed vs Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Model Efficiency\n",
    "model_names = [data['name'] for data in performance_data.values()]\n",
    "efficiencies = [data['efficiency'] for data in performance_data.values()]\n",
    "colors = [data['color'] for data in performance_data.values()]\n",
    "\n",
    "bars = ax3.bar(range(len(model_names)), efficiencies, color=colors, alpha=0.8)\n",
    "ax3.set_xlabel('Models', fontsize=12)\n",
    "ax3.set_ylabel('Accuracy per Million Parameters', fontsize=12)\n",
    "ax3.set_title('üìä Model Efficiency', fontsize=14, fontweight='bold')\n",
    "ax3.set_xticks(range(len(model_names)))\n",
    "ax3.set_xticklabels([f\"{data['rank']}\" for data in performance_data.values()])\n",
    "\n",
    "# 4. Summary Table\n",
    "ax4.axis('off')\n",
    "table_data = []\n",
    "for model_key, data in performance_data.items():\n",
    "    table_data.append([\n",
    "        f\"{data['rank']} {data['name']}\",\n",
    "        f\"{data['accuracy']:.1f}%\",\n",
    "        f\"{data['parameters']:.1f}M\",\n",
    "        f\"{data['inference_time']:.1f}ms\",\n",
    "        f\"{data['input_size']}\"\n",
    "    ])\n",
    "\n",
    "table = ax4.table(cellText=table_data,\n",
    "                  colLabels=['Model', 'Accuracy', 'Parameters', 'Inference', 'Input Size'],\n",
    "                  cellLoc='center',\n",
    "                  loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.5)\n",
    "ax4.set_title('üìã Performance Summary', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.suptitle('‚ö° Performance Analysis - Top Models', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n‚ö° Performance Summary:\")\n",
    "print(\"=\"*50)\n",
    "for model_key, data in performance_data.items():\n",
    "    print(f\"{data['rank']} {data['name']}:\")\n",
    "    print(f\"   üéØ Accuracy: {data['accuracy']:.2f}%\")\n",
    "    print(f\"   üìê Input Size: {data['input_size']}\")\n",
    "    print(f\"   ‚öôÔ∏è Parameters: {data['parameters']:.1f}M\")\n",
    "    print(f\"   ‚ö° Inference: {data['inference_time']:.1f} ¬± {data['inference_std']:.1f} ms/image\")\n",
    "    print(f\"   üìä Efficiency: {data['efficiency']:.1f} acc/M params\")\n",
    "    print()\n",
    "\n",
    "print(\"üèÜ Winners:\")\n",
    "best_acc = max(performance_data.values(), key=lambda x: x['accuracy'])\n",
    "fastest = min(performance_data.values(), key=lambda x: x['inference_time'])\n",
    "most_efficient = max(performance_data.values(), key=lambda x: x['efficiency'])\n",
    "\n",
    "print(f\"ü•á Best Accuracy: {best_acc['name']} ({best_acc['accuracy']:.2f}%)\")\n",
    "print(f\"‚ö° Fastest: {fastest['name']} ({fastest['inference_time']:.1f} ms)\")\n",
    "print(f\"üìä Most Efficient: {most_efficient['name']} ({most_efficient['efficiency']:.1f})\")\n",
    "\n",
    "print(f\"\\n‚úÖ Performance Analysis Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee4487c",
   "metadata": {},
   "source": [
    "## üìà 5. Final Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06942b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéä COMPREHENSIVE FINAL ANALYSIS & ACTIONABLE INSIGHTS\n",
    "print(\"üéä COMPREHENSIVE MODEL ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create comprehensive comparison visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Model Accuracy Comparison\n",
    "if 'model_info' in locals():\n",
    "    model_names = [info['name'] for info in model_info.values()]\n",
    "    accuracies = [info['accuracy'] for info in model_info.values()]\n",
    "    colors = [info['color'] for info in model_info.values()]\n",
    "    ranks = [info['rank'] for info in model_info.values()]\n",
    "    \n",
    "    bars1 = ax1.bar(range(len(model_names)), accuracies, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "    ax1.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('üèÜ Model Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xticks(range(len(model_names)))\n",
    "    ax1.set_xticklabels([f\"{rank}\" for rank in ranks], fontsize=12)\n",
    "    ax1.set_ylim([94, 97])\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, acc in zip(bars1, accuracies):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "                 f'{acc:.2f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# 2. Parameter Efficiency\n",
    "if 'model_info' in locals():\n",
    "    param_counts = [info['parameters'] / 1e6 for info in model_info.values()]\n",
    "    efficiency = [acc / param for acc, param in zip(accuracies, param_counts)]\n",
    "    \n",
    "    bars2 = ax2.bar(range(len(model_names)), efficiency, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "    ax2.set_ylabel('Accuracy per Million Parameters', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('üìä Parameter Efficiency', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xticks(range(len(model_names)))\n",
    "    ax2.set_xticklabels([f\"{rank}\" for rank in ranks], fontsize=12)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, eff in zip(bars2, efficiency):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                 f'{eff:.1f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# 3. Input Size Requirements\n",
    "input_sizes = ['224√ó224\\n(VGG16)', '256√ó256\\n(Custom CNN)']\n",
    "input_colors = ['#FF6B6B', '#4ECDC4']\n",
    "size_values = [224, 256]\n",
    "\n",
    "bars3 = ax3.bar(range(len(input_sizes)), size_values, color=input_colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax3.set_ylabel('Input Resolution (pixels)', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('üìê Model Input Requirements', fontsize=14, fontweight='bold')\n",
    "ax3.set_xticks(range(len(input_sizes)))\n",
    "ax3.set_xticklabels(input_sizes, fontsize=10)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotations\n",
    "ax3.annotate('ImageNet\\nStandard', xy=(0, 224), xytext=(0, 280),\n",
    "             ha='center', fontsize=9, \n",
    "             arrowprops=dict(arrowstyle='->', color='red', alpha=0.7))\n",
    "ax3.annotate('Higher\\nResolution', xy=(1, 256), xytext=(1, 320),\n",
    "             ha='center', fontsize=9,\n",
    "             arrowprops=dict(arrowstyle='->', color='green', alpha=0.7))\n",
    "\n",
    "# 4. Deployment Decision Matrix\n",
    "ax4.axis('off')\n",
    "\n",
    "# Create decision matrix\n",
    "decision_data = [\n",
    "    ['Metric', 'ü•á VGG16', 'ü•à Custom CNN', 'Winner'],\n",
    "    ['Accuracy', '96.22%', '95.80%', 'ü•á VGG16'],\n",
    "    ['Parameters', '15.1M', '1.6M', 'ü•à Custom CNN'],\n",
    "    ['Efficiency', '6.4', '59.9', 'ü•à Custom CNN'],\n",
    "    ['Input Size', '224√ó224', '256√ó256', 'Depends on use case'],\n",
    "    ['Best For', 'Max Accuracy', 'Speed/Efficiency', '-']\n",
    "]\n",
    "\n",
    "table = ax4.table(cellText=decision_data[1:],\n",
    "                  colLabels=decision_data[0],\n",
    "                  cellLoc='center',\n",
    "                  loc='center',\n",
    "                  colColours=['lightblue', '#FFE5E5', '#E5F5F5', '#F0F0F0'])\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 2)\n",
    "\n",
    "# Style the table\n",
    "for i in range(len(decision_data)):\n",
    "    for j in range(len(decision_data[0])):\n",
    "        cell = table[(i, j)]\n",
    "        if i == 0:  # Header row\n",
    "            cell.set_text_props(weight='bold')\n",
    "        if j == 3:  # Winner column\n",
    "            cell.set_text_props(weight='bold')\n",
    "\n",
    "ax4.set_title('üéØ Deployment Decision Matrix', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.suptitle('üìä COMPREHENSIVE MODEL ANALYSIS DASHBOARD\\nKey Insights for Production Deployment', \n",
    "             fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Generate actionable recommendations\n",
    "print(\"\\nüéØ ACTIONABLE DEPLOYMENT RECOMMENDATIONS:\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "print(\"üìã SCENARIO-BASED DEPLOYMENT GUIDE:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "scenarios = {\n",
    "    \"\udfe5 Healthcare/Critical Systems\": {\n",
    "        \"recommendation\": \"VGG16 Transfer Learning\",\n",
    "        \"reason\": \"Maximum accuracy (96.22%) critical for safety\",\n",
    "        \"setup\": \"224√ó224 input, GPU recommended\"\n",
    "    },\n",
    "    \"üì± Mobile/Edge Applications\": {\n",
    "        \"recommendation\": \"Custom CNN\", \n",
    "        \"reason\": \"9.5x fewer parameters, faster inference\",\n",
    "        \"setup\": \"256√ó256 input, CPU/mobile GPU friendly\"\n",
    "    },\n",
    "    \"üåê Web Applications\": {\n",
    "        \"recommendation\": \"Custom CNN\",\n",
    "        \"reason\": \"Balance of accuracy (95.80%) and speed\",\n",
    "        \"setup\": \"256√ó256 input, API deployment ready\"\n",
    "    },\n",
    "    \"üöó Real-time Vehicle Systems\": {\n",
    "        \"recommendation\": \"Custom CNN\",\n",
    "        \"reason\": \"Sub-100ms inference critical\",\n",
    "        \"setup\": \"256√ó256 input, embedded systems\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for scenario, details in scenarios.items():\n",
    "    print(f\"\\n{scenario}:\")\n",
    "    print(f\"   ‚úÖ Recommended: {details['recommendation']}\")\n",
    "    print(f\"   üìù Reason: {details['reason']}\")\n",
    "    print(f\"   ‚öôÔ∏è Setup: {details['setup']}\")\n",
    "\n",
    "print(f\"\\nüîß IMPLEMENTATION CHECKLIST:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"‚úÖ Model files available in deployment_models/\")\n",
    "print(\"‚úÖ Input size handling implemented\")\n",
    "print(\"‚úÖ Performance benchmarks completed\")\n",
    "print(\"‚úÖ Accuracy validation performed\")\n",
    "print(\"‚úÖ Memory requirements documented\")\n",
    "\n",
    "print(f\"\\nüìà PERFORMANCE SUMMARY:\")\n",
    "print(\"-\" * 25)\n",
    "if 'metrics_summary' in locals():\n",
    "    for model_key, metrics in metrics_summary.items():\n",
    "        info = model_info[model_key]\n",
    "        print(f\"{info['rank']} {info['name']}:\")\n",
    "        print(f\"   üéØ Test Accuracy: {metrics['accuracy']:.2f}%\")\n",
    "        print(f\"   ‚öñÔ∏è Precision: {metrics['precision']:.3f}\")\n",
    "        print(f\"   üîÑ Recall: {metrics['recall']:.3f}\")\n",
    "        print(f\"   üìä F1-Score: {metrics['f1']:.3f}\")\n",
    "\n",
    "print(f\"\\nüöÄ NEXT STEPS:\")\n",
    "print(\"-\" * 15)\n",
    "print(\"1. Choose model based on deployment scenario\")\n",
    "print(\"2. Set up preprocessing pipeline with correct input size\")\n",
    "print(\"3. Deploy with appropriate hardware specifications\")\n",
    "print(\"4. Monitor performance in production\")\n",
    "print(\"5. Set up A/B testing if needed\")\n",
    "\n",
    "print(f\"\\nüéâ ANALYSIS COMPLETE!\")\n",
    "print(\"Ready for production deployment with data-driven model selection.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
